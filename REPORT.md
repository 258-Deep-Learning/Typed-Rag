Typed-RAG: Type-Aware Decomposition of Non-Factoid Questions for RAG
Project Report

Cover Information
Team ID: Team 15
Project Title: Typed-RAG: Type-Aware Decomposition of Non-Factoid Questions for Retrieval-Augmented Generation
Team Members:
- Indraneel Sarode, 018305092, indraneel.sarode@sjsu.edu
- Nishan Paudel, 018280561, nishan.paudel@sjsu.edu
- Vatsal Gandhi, 018274841, vatsal.gandhi@sjsu.edu 

Project Track: System
Focused Areas:
Retrieval-Augmented Generation (RAG)
Non-Factoid Question Answering (NFQA)
Query Decomposition
Natural Language Processing
Vector Search and Dense Retrieval

Abstract
Standard Retrieval-Augmented Generation (RAG) systems excel at answering simple factoid questions but struggle with complex, non-factoid questions (NFQs) that require multi-aspect reasoning, diverse perspectives, and structured synthesis. Questions like â€œPython vs. Java for web development?â€ or â€œPros and cons of remote workâ€ demand analysis across multiple dimensions rather than retrieval of a single fact. This project implements Typed-RAG, a novel framework that integrates question type classification directly into the retrieval pipeline to address these challenges.
Our approach classifies questions into six distinct types (Evidence-based, Comparison, Experience, Reason, Instruction, Debate) and decomposes them into targeted sub-queries tailored to each typeâ€™s characteristics. We developed a complete end-to-end system using Python, FAISS/Pinecone for vector search, BGE embeddings, and Google Gemini for generation. The system processes questions through a five-stage pipeline: classification, type-aware decomposition, aspect-level retrieval, generation, and structured aggregation.
Comprehensive evaluation on 97 questions from the Wiki-NFQA dataset demonstrates that our Typed-RAG system achieves a Mean Reciprocal Rank (MRR) of 0.2880 and Mean Percentile Rank (MPR) of 62.89% using Llama 3.3 70B, with 100% success rate across all question types. We evaluated six different system configurations (3 approaches Ã— 2 models) and found that Typed-RAG outperforms standard RAG baseline by 5.17% MPR for Gemini and shows strong performance on DEBATE (79.17% MPR) and COMPARISON (80.00% MPR) question types. Interestingly, LLM-only approaches achieved the highest scores (Llama: 71.90% MPR), revealing that retrieval quality significantly impacts RAG effectiveness. The system achieves an average end-to-end latency of 5.03 seconds per question with 91.75% classification accuracy, demonstrating practical real-world applicability.

1. Introduction & Problem Description
1.1 Problem Statement
Traditional Retrieval-Augmented Generation (RAG) systems treat all queries identicallyâ€”embedding the question, retrieving the top-k most similar documents, and passing them to a language model for answer generation. While this approach works well for factoid questions (e.g., â€œWhen was Google founded?â€ â†’ â€œSeptember 4, 1998â€), it fundamentally fails for non-factoid questions (NFQs) that require:
Multi-aspect reasoning: Comparison questions need analysis across multiple dimensions (performance, cost, usability)
Diverse perspectives: Debate questions require presenting opposing viewpoints
Sequential structure: Instruction questions demand step-by-step organization
Causal analysis: Reason questions need exploration of causes and mechanisms
Example of Failure:
Question: â€œPython vs Java for web development?â€
Naive RAG: Retrieves 5 documents about â€œPython vs Javaâ€, which may all focus on performance, missing critical aspects like ecosystem, learning curve, or deployment options
User Need: Comprehensive comparison across multiple axes with structured presentation
1.2 Importance and Impact
As Large Language Models become primary knowledge assistants, users increasingly ask complex, exploratory questions rather than simple fact lookups. According to recent studies:
60-70% of real-world queries to knowledge systems are non-factoid in nature
Users expect comprehensive, well-structured answers similar to high-quality Wikipedia articles
Standard RAG systems produce incomplete or poorly organized responses for these questions
Real-World Applications:
Technical Support: â€œHow do I deploy a machine learning model to production?â€ (Instruction)
Product Research: â€œMacBook vs ThinkPad for software development?â€ (Comparison)
Educational Tools: â€œWhy did the Industrial Revolution start in Britain?â€ (Reason)
Decision Support: â€œShould companies adopt microservices architecture?â€ (Debate)
1.3 Target Users and Application Scenarios
Primary Users:
Students and Researchers: Exploring complex topics requiring multi-dimensional analysis
Software Developers: Seeking comprehensive technical guidance and best practices
Business Professionals: Making informed decisions based on structured comparisons
Content Creators: Generating well-organized educational material
Application Scenarios:
Enterprise knowledge bases with technical documentation
Educational platforms requiring comprehensive explanations
Customer support systems handling complex troubleshooting
Research assistants for literature review and synthesis

2. Background / Related Work
2.1 Research Foundation
This project is based on the paper â€œTyped-RAG: Type-Aware Decomposition of Non-Factoid Questions for Retrieval-Augmented Generationâ€ by Park et al. (2025), which introduces the concept of question type classification integrated into the RAG paradigm.
Key References:
Retrieval-Augmented Generation (RAG)
Lewis et al. (2020): â€œRetrieval-Augmented Generation for Knowledge-Intensive NLP Tasksâ€
Introduced the RAG paradigm combining retrieval with generation
Demonstrated improvements over pure LLM approaches on factoid QA
Non-Factoid Question Answering
Bolotova et al. (2022): Challenges in NFQA systems
Yang & Alonso (2024): Prevalence of NFQs in real-world search
An et al. (2024): Type-specific approaches to NFQA
Query Decomposition
Least-to-Most Prompting (Zhou et al., 2023)
Self-Ask (Press et al., 2022)
Difference: Our approach uses a fixed taxonomy for consistency
2.2 Technology Stack
State-of-the-Art Models and Libraries:
Embeddings: BGE (BAAI General Embedding)
Model: BAAI/bge-small-en-v1.5
Dimensions: 384
Performance: Ranked #1 on MTEB leaderboard for small models
Speed: ~1000 embeddings/second on CPU
Vector Stores:
FAISS (Facebook AI Similarity Search): Local, high-performance similarity search
Pinecone: Cloud-based vector database with built-in scaling
Language Models:
Primary: Google Gemini 2.5 Flash (commercial, unlimited quota)
Alternative: Meta Llama 3.3 70B via Groq (open-source, 30 req/min)
Used for: Question answering, classification fallback, answer aggregation
Reranking: Cross-encoder models from sentence-transformers
Frameworks:
LangChain: For orchestration and document processing
Transformers: HuggingFace library for model inference
Sentence-Transformers: For embeddings and reranking
2.3 What Makes This Different
Unlike generic query decomposition approaches:
Fixed Question Taxonomy: 6 predefined types with proven effectiveness
Type-Specific Strategies: Each type has tailored decomposition logic
Aspect-Level Retrieval: Retrieve evidence independently for each sub-query
Structured Aggregation: Type-aware formatting of final answers
Comparison to Existing Work:
Approach
Question Types
Decomposition
Retrieval
Aggregation
Standard RAG
Agnostic
None
Single query
Simple concat
Self-Ask
Agnostic
Chain prompting
Sequential
Sequential
Typed-RAG
6 types
Type-aware
Parallel
Structured


3. System / Model / Algorithm Design
3.1 Overall Architecture

The Typed-RAG system implements a five-stage pipeline where each stage is designed to be modular, cacheable, and testable:

**User Question â†’ Classification â†’ Decomposition â†’ Retrieval â†’ Generation â†’ Aggregation â†’ Final Answer**

**[INSERT IMAGE: Figure 3.1 - Typed-RAG System Architecture - Five-Stage Pipeline]**

ASCII Architecture Diagram:



â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  User Question   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚1. CLASSIFIER  â”‚  â† Hybrid: Pattern Matching + LLM Fallback
â”‚(Question Type)â”‚ â†’ Evidence,Comparison,Reason,Instruction,Experience, Debate
â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  2. DECOMPOSER         â”‚  â† Type-Aware Strategy
â”‚  (Sub-Queries)         â”‚  â†’ 1-5 aspect-level queries
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  3. RETRIEVAL ORCHESTRATOR   â”‚  â† BGE Embeddings + FAISS/Pinecone
â”‚  (Evidence per Aspect)       â”‚  â†’ Top-5 documents per sub-query
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  4. ASPECT GENERATOR   â”‚  â† LLM (Gemini) per aspect
â”‚  (Aspect Answers)      â”‚  â†’ Individual aspect responses
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  5. AGGREGATOR         â”‚  â† Type-aware formatting
â”‚  (Final Answer)        â”‚  â†’ Structured comprehensive response
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
3.2 Component Design
3.2.1 Question Classifier
Purpose: Determine the question type to enable appropriate decomposition strategy.
Implementation: Hybrid approach balancing speed, accuracy, and cost
Architecture:
Question â†’ Pattern Matcher â†’ Match Found? 
                    â†“ No
           LLM Classifier (Gemini)
                    â†“
              Question Type

Pattern Matching Examples:
Evidence: â€œWhat isâ€¦â€, â€œDefineâ€¦â€, â€œExplainâ€¦â€
Comparison: â€œvsâ€, â€œcompared toâ€, â€œdifference betweenâ€
Reason: â€œWhyâ€¦â€, â€œWhat causesâ€¦â€, â€œHow comeâ€¦â€
Instruction: â€œHow toâ€¦â€, â€œSteps toâ€¦â€, â€œGuide forâ€¦â€
Experience: â€œexperiences withâ€, â€œopinions onâ€, â€œreviews ofâ€
Debate: â€œpros and consâ€, â€œadvantages and disadvantagesâ€
Performance:
Pattern matching: <1ms, 67% coverage
LLM fallback: 200-500ms, 100% accuracy
Hybrid accuracy: 90-95%
3.2.2 Query Decomposer
Purpose: Break down complex questions into focused sub-queries based on question type.
Type-Specific Strategies:
Evidence-based: Passthrough (1 query)
Q: "What is quantum computing?"
â†’ Sub-queries: ["What is quantum computing?"]
Comparison: Extract subjects + generate comparison axes (3-5 queries)
Q: "Python vs Java for web development?"
â†’ Subjects: [Python, Java]
â†’ Axes: [Performance, Ecosystem, Learning Curve, Deployment, Community]
â†’ Sub-queries: 
   - "Performance comparison: Python vs Java for web development"
   - "Ecosystem comparison: Python vs Java web frameworks"
   - "Learning curve: Python vs Java for web developers"
   - "Deployment options: Python vs Java web applications"
   - "Community support: Python vs Java web development"
Reason: Causal decomposition (2-4 queries)
Q: "Why is React popular for frontend development?"
â†’ Sub-queries:
   - "Direct causes: What makes React popular?"
   - "Mechanisms: How does React work differently?"
   - "Historical context: React's evolution and adoption"
   - "Constraints: React's limitations and trade-offs"
Instruction: Sequential steps (3-5 queries)
Q: "How to deploy a machine learning model to production?"
â†’ Sub-queries:
   - "Prerequisites for ML model deployment"
   - "Model preparation and serialization"
   - "Infrastructure setup and deployment options"
   - "Monitoring and validation post-deployment"
   - "Common deployment issues and troubleshooting"
Experience: Topical angles (2-4 queries)
Q: "What are developers' experiences with Rust?"
â†’ Sub-queries:
   - "Reliability and code quality experiences with Rust"
   - "Performance characteristics: developer observations"
   - "Learning curve and productivity with Rust"
   - "Ecosystem maturity and tooling experiences"
Debate: Opposing viewpoints + synthesis (3 queries)
Q: "Should companies adopt microservices architecture?"
â†’ Sub-queries:
   - "Arguments in favor of microservices adoption"
   - "Arguments against microservices adoption"
   - "Balanced analysis: microservices trade-offs"
Output Format:
{
  "question_id": "abc123",
  "original_question": "Python vs Java for web development?",
  "type": "Comparison",
  "sub_queries": [
    {
      "aspect": "performance",
      "query": "Performance comparison: Python vs Java",
      "strategy": "compare"
    },
    ...
  ]
}
3.2.3 Retrieval Orchestrator
Purpose: Retrieve relevant evidence for each sub-query with optional reranking.
Pipeline:
Sub-Query â†’ BGE Embedder â†’ Dense Retrieval (FAISS/Pinecone, k=50)
                                    â†“
                          Reranking? â†’ Cross-Encoder â†’ Top-5 Documents
                                    â†“ (optional)
                              Evidence Bundle
Dense Retrieval:
Embedding Model: BGE-small-en-v1.5 (384 dimensions)
Index Type: FAISS (L2 similarity) or Pinecone (cosine similarity)
Initial Retrieval: Top-50 documents per sub-query
Optional Reranking:
Model: Cross-encoder (sentence-transformers)
Purpose: Refine relevance scores using bi-encoder architecture
Performance: +10-15% precision, +50-100ms latency
Caching Strategy:
Cache key: Hash(sub-query + vector_store_config)
Location: cache/evidence/{hash}.json
Benefits: Eliminates redundant retrievals during development/testing
Output:
{
  "evidence": [
    {
      "aspect": "performance",
      "documents": [
        {
          "id": "doc_001",
          "text": "...",
          "score": 0.89,
          "source": "wikipedia"
        }
      ]
    }
  ]
}
3.2.4 Answer Generator & Aggregator
Aspect Generator:
Generates focused answer for each aspect using evidence bundle
Uses LLM (Gemini) with aspect-specific prompts
Includes citations to retrieved documents
Fallback: Template-based generation when LLM unavailable
Aggregator:
Combines aspect answers into coherent final response
Applies type-specific formatting:
Comparison: Side-by-side structure
Instruction: Numbered steps
Debate: Pro/Con sections with synthesis
Reason: Causal chain explanation
3.3 Key Algorithms
Algorithm 1: Hybrid Classification
def classify_question(question: str, use_llm: bool = True) -> str:
    # Step 1: Pattern matching (fast path)
    for qtype, patterns in PATTERNS.items():
        for pattern in patterns:
            if re.search(pattern, question, re.IGNORECASE):
                return qtype
    
    # Step 2: LLM fallback (slow path)
    if use_llm:
        return llm_classify(question)
    
    # Step 3: Default fallback
    return "Evidence-based"
Algorithm 2: Type-Aware Decomposition
def decompose_question(question: str, qtype: str) -> DecompositionPlan:
    if qtype == "Comparison":
        subjects = extract_subjects(question)
        axes = generate_comparison_axes(subjects)
        sub_queries = [
            f"{axis} comparison: {subjects[0]} vs {subjects[1]}"
            for axis in axes
        ]
    elif qtype == "Reason":
        sub_queries = [
            f"Direct causes: {question}",
            f"Mechanisms: How {question}",
            f"Historical context: {question}",
            f"Constraints: Limitations of {question}"
        ]
    # ... other types
    
    return DecompositionPlan(
        original_question=question,
        question_type=qtype,
        sub_queries=sub_queries
    )
Algorithm 3: Evidence Retrieval with Reranking
def retrieve_evidence(sub_query: str, rerank: bool = False):
    # Stage 1: Dense retrieval
    query_embedding = embedder.embed(sub_query)
    candidates = vector_store.search(query_embedding, k=50)
    
    # Stage 2: Optional reranking
    if rerank:
        scores = cross_encoder.predict([
            (sub_query, doc.text) for doc in candidates
        ])
        candidates = rerank_by_scores(candidates, scores)
    
    return candidates[:5]  # Final top-5

4. Implementation Details
4.1 Technology Stack
Programming Language: Python 3.10+
Core Dependencies:
sentence-transformers==2.2.2    # BGE embeddings + cross-encoder
faiss-cpu==1.7.4               # Local vector search
pinecone-client==2.2.4         # Cloud vector search
google-generativeai==0.3.2     # Gemini API
langchain==0.1.0               # Document processing
transformers==4.36.0           # Model loading
streamlit==1.29.0              # Web UI
pytest==7.4.3                  # Testing framework
Full Requirements: See requirements.txt in repository
4.2 System Setup
Development Environment:
OS: macOS (Darwin 25.1.0) / Linux
Python: 3.13.7 (tested), 3.10+ compatible
Memory: 8GB minimum, 16GB recommended
Disk: ~5GB for models and indexes
Hardware Recommendations:
CPU: Modern multi-core processor (no GPU required for inference)
RAM: 16GB for comfortable development with caching
Storage: SSD recommended for faster index loading
Deployment Options:
Local: FAISS + CPU inference (fully offline after setup)
Hybrid: Local retrieval + Cloud LLM (Gemini API)
Cloud: Pinecone + Gemini (fully cloud-based)
4.3 Project Structure
Typed-Rag/
â”œâ”€â”€ typed_rag/                    # Main package
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ classifier/               # Question classification
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ classifier.py         # Hybrid classifier implementation
â”‚   â”œâ”€â”€ decompose/                # Query decomposition
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â””â”€â”€ query_decompose.py    # Type-aware decomposition
â”‚   â”œâ”€â”€ retrieval/                # Retrieval engine
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ pipeline.py           # BGE embedder + vector stores
â”‚   â”‚   â””â”€â”€ orchestrator.py       # Multi-query orchestration
â”‚   â”œâ”€â”€ generation/               # Answer generation
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ generator.py          # Aspect-level generation
â”‚   â”‚   â”œâ”€â”€ aggregator.py         # Final answer aggregation
â”‚   â”‚   â””â”€â”€ prompts.py            # LLM prompts
â”‚   â”œâ”€â”€ eval/                     # Evaluation framework
â”‚   â”‚   â”œâ”€â”€ __init__.py
â”‚   â”‚   â”œâ”€â”€ linkage.py            # LINKAGE metrics
â”‚   â”‚   â”œâ”€â”€ references.py         # Reference answer handling
â”‚   â”‚   â””â”€â”€ runner.py             # Evaluation runner
â”‚   â”œâ”€â”€ scripts/                  # Index building scripts
â”‚   â”‚   â”œâ”€â”€ build_faiss.py        # FAISS index builder
â”‚   â”‚   â”œâ”€â”€ build_pinecone.py     # Pinecone uploader
â”‚   â”‚   â””â”€â”€ ingest_own_docs.py    # Document ingestion
â”‚   â”œâ”€â”€ data/                     # Data loaders
â”‚   â”‚   â””â”€â”€ loaders.py
â”‚   â”œâ”€â”€ core/                     # Core utilities
â”‚   â”‚   â””â”€â”€ keys.py               # API key management
â”‚   â””â”€â”€ rag_system.py             # Main orchestration
â”‚
â”œâ”€â”€ scripts/                      # Evaluation scripts
â”‚   â”œâ”€â”€ run_typed_rag.py          # Run full system
â”‚   â”œâ”€â”€ run_rag_baseline.py       # Baseline comparison
â”‚   â”œâ”€â”€ run_llm_only.py           # LLM-only baseline
â”‚   â”œâ”€â”€ run_ablation_study.py     # Ablation experiments
â”‚   â”œâ”€â”€ evaluate_linkage.py       # Quality evaluation
â”‚   â”œâ”€â”€ profile_performance.py    # Performance profiling
â”‚   â”œâ”€â”€ create_evaluation_plots.py # Visualization
â”‚   â””â”€â”€ setup_wiki_nfqa.py        # Dataset setup
â”‚
â”œâ”€â”€ tests/                        # Unit & integration tests
â”‚   â”œâ”€â”€ test_classifier.py        # Classification tests
â”‚   â”œâ”€â”€ test_decomposition.py     # Decomposition tests (30 tests)
â”‚   â””â”€â”€ test_generation.py        # Generation tests
â”‚
â”œâ”€â”€ examples/                     # Example scripts
â”‚   â”œâ”€â”€ typed_rag_pipeline.py     # Complete pipeline demo
â”‚   â”œâ”€â”€ happy_flow_demo.py        # Multi-type demo
â”‚   â””â”€â”€ metrics_tracker.py        # Metrics tracking
â”‚
â”œâ”€â”€ data/                         # Datasets
â”‚   â”œâ”€â”€ wiki_nfqa/                # Wiki-NFQA dataset
â”‚   â”‚   â””â”€â”€ dev6.jsonl            # 6-question dev set
â”‚   â””â”€â”€ passages.jsonl            # Wikipedia passages
â”‚
â”œâ”€â”€ indexes/                      # Vector indexes
â”‚   â””â”€â”€ wikipedia/
â”‚       â””â”€â”€ faiss/
â”‚           â”œâ”€â”€ index.faiss       # FAISS index file
â”‚           â””â”€â”€ index.pkl         # Document metadata
â”‚
â”œâ”€â”€ cache/                        # Runtime cache (auto-generated)
â”‚   â”œâ”€â”€ decomposition/            # Cached decomposition plans
â”‚   â”œâ”€â”€ evidence/                 # Cached evidence bundles
â”‚   â”œâ”€â”€ answers/                  # Cached aspect answers
â”‚   â””â”€â”€ final_answers/            # Cached final answers
â”‚
â”œâ”€â”€ results/                      # Evaluation outputs
â”‚   â”œâ”€â”€ ablation/                 # Ablation study results
â”‚   â””â”€â”€ plots/                    # Generated visualizations
â”‚
â”œâ”€â”€ docs/                         # Documentation
â”‚   â”œâ”€â”€ architecture.md           # Architecture diagrams
â”‚   â””â”€â”€ screenshots.md            # UI screenshots guide
â”‚
â”œâ”€â”€ app.py                        # Streamlit web UI
â”œâ”€â”€ main.py                       # CLI entry point
â”œâ”€â”€ rag_cli.py                    # Command-line interface
â”œâ”€â”€ ask.py                        # Simple query interface
â”œâ”€â”€ requirements.txt              # Python dependencies
â”œâ”€â”€ README_TYPED_RAG.md           # Main documentation
â”œâ”€â”€ TESTING.md                    # Testing guide
â”œâ”€â”€ EVALUATION.md                 # Evaluation guide
â””â”€â”€ paper.txt                     # Research paper

4.4 Key Implementation Decisions
4.4.1 Caching Architecture
Motivation: Minimize API costs and improve development iteration speed
Implementation:
Cache Key: MD5 hash of (question + configuration)
Storage: JSON files in cache/ directories
Layers:
Decomposition plans
Evidence bundles
Aspect answers
Final aggregated answers
Impact:
Development: 10x faster iteration (no repeated API calls)
Testing: Deterministic outputs for unit tests
Cost: Reduced API usage by ~80% during development
4.4.2 Fallback Mechanisms
Problem: LLM APIs can fail or be unavailable
Solution: Multi-level fallback strategy
# Level 1: LLM-based (best quality)
if use_llm and api_available():
    return llm_generate(prompt)

# Level 2: Heuristic-based (good quality)
elif has_evidence:
    return template_generate(evidence)

# Level 3: Deterministic fallback (basic quality)
else:
    return "Unable to answer: insufficient information"
Result: System never crashes, always produces valid output
4.4.3 Modular Design
Principle: Each component is independently testable and replaceable
Benefits:
Testing: Can test classifier without touching retrieval
Development: Team members can work on different components
Experimentation: Easy to swap embedding models or LLMs
Example - Swapping Vector Stores:
# FAISS (local)
vector_store = load_faiss_adapter("indexes/faiss")

# Pinecone (cloud)
vector_store = PineconeDenseStore(index_name="typed-rag")

# Both implement same interface â†’ no code changes needed
4.5 Code Repository

**GitHub Repository**: https://github.com/258-Deep-Learning/Typed-Rag

**Installation:**
```bash
# Clone repository
git clone https://github.com/258-Deep-Learning/Typed-Rag.git
cd Typed-Rag

# Create virtual environment
python3 -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt

# Set up API keys
export GOOGLE_API_KEY="your-gemini-api-key"
Quick Start:
# Build FAISS index
python rag_cli.py build --backend faiss --source wikipedia

# Ask a question
python rag_cli.py typed-ask "Python vs Java for web development?"

# Run web UI
streamlit run app.py

5. Task Distribution & Contributions
5.1 Team Contributions
Team Member
Primary Responsibilities
Components Owned
Contribution %
Indraneel Sarode
Classification & Decomposition Logic
classifier.py, query_decompose.py, pattern matching, prompts
33%
Nishan Paudel
Retrieval Engine & Vector Search
pipeline.py, FAISS/Pinecone integration, embeddings, reranking
33%
Vatsal Gandhi
System Integration, Evaluation & UI
rag_system.py, orchestrator.py, eval/, app.py, CLI tools
34%

5.2 Detailed Breakdown
[Member 1]: System Architecture & Integration
Modules:
typed_rag/rag_system.py (480 lines)
typed_rag/retrieval/orchestrator.py (320 lines)
rag_cli.py (250 lines)
main.py (150 lines)
Key Contributions:
Designed the 5-stage pipeline architecture
Implemented DataType abstraction for backend/source selection
Created IndexBuilder and QueryEngine separation of concerns
Built CLI interface with typed-ask command
Integrated all components into cohesive system
Code Complexity: High - Required understanding of entire system flow
Indraneel Sarode: Classification & Decomposition
Modules:
- typed_rag/classifier/classifier.py (280 lines)
- typed_rag/decompose/query_decompose.py (450 lines)
- typed_rag/generation/prompts.py (200 lines)
- Pattern database and regex definitions
Key Contributions:
- Implemented hybrid classifier (pattern matching + LLM fallback)
- Designed 60+ regex patterns for 6 question types
- Created type-specific decomposition strategies for all 6 types
- Developed comparison axis generation algorithm
- Built aspect extraction for Evidence, Reason, and Experience types
- Engineered LLM prompts for classification and decomposition
- Implemented caching system for decomposition plans
- Achieved 91.75% classification accuracy
Code Complexity: High - Required linguistic analysis, pattern design, and LLM prompt engineering
Lines of Code: ~930 lines
Nishan Paudel: Retrieval & Vector Search
Modules:
- typed_rag/retrieval/pipeline.py (380 lines)
- typed_rag/retrieval/orchestrator.py (320 lines)
- typed_rag/scripts/build_faiss.py (200 lines)
- typed_rag/scripts/build_pinecone.py (180 lines)
- FAISS/Pinecone adapter implementations
Key Contributions:
- Integrated BGE embedding model (BAAI/bge-small-en-v1.5)
- Implemented FAISS index building pipeline for Wikipedia passages
- Created Pinecone cloud vector store integration
- Built cross-encoder reranking system for relevance refinement
- Developed multi-query retrieval orchestration (parallel sub-query processing)
- Optimized retrieval performance with batch embedding
- Implemented evidence caching system
- Achieved 31.4% of total latency for retrieval (efficient)
Code Complexity: Medium-High - Required understanding of dense retrieval, vector search algorithms, and optimization
Lines of Code: ~1080 lines
Vatsal Gandhi: System Integration, Evaluation & UI
Modules:
- typed_rag/rag_system.py (480 lines)
- typed_rag/eval/ (4 files, 600 lines total)
- typed_rag/generation/aggregator.py (280 lines)
- scripts/run_typed_rag.py (350 lines)
- scripts/run_ablation_study.py (350 lines)
- scripts/evaluate_linkage.py (420 lines)
- scripts/create_evaluation_plots.py (420 lines)
- tests/ (3 files, 500 lines)
- app.py (480 lines)
- rag_cli.py (250 lines)
- main.py (150 lines)
Key Contributions:
- Designed and implemented 5-stage pipeline architecture
- Built end-to-end system integration (classification â†’ decomposition â†’ retrieval â†’ generation â†’ aggregation)
- Implemented LINKAGE evaluation metrics (MRR, MPR) for quality assessment
- Created comprehensive test suite (33+ unit tests, 5 per question type)
- Developed ablation study framework with 4 variants and incremental save/resume
- Built Streamlit web interface with 3 tabs (Query, Evaluation, Architecture)
- Engineered answer aggregator with type-aware formatting
- Generated publication-quality evaluation plots and visualizations
- Wrote CLI tools (rag_cli.py, main.py) for system operation
- Conducted full evaluation: 6 systems Ã— 97 questions = 582 answers
- Performance profiling and latency analysis
Code Complexity: High - Required system design, integration, metrics implementation, and full-stack development
Lines of Code: ~4,280 lines
5.3 Collaboration & Integration
Development Process:
Week 1-2: Architecture design and component interface definition
Week 3-4: Parallel development of individual components
Week 5: Integration and end-to-end testing
Week 6: Evaluation, optimization, and documentation
Tools Used:
Version Control: Git + GitHub
Communication: Slack for daily updates
Documentation: Markdown in repository
Testing: pytest with CI/CD pipeline
Code Reviews:
Each PR required approval from 1 other team member
Focus on maintainability and documentation
Ensured consistent code style across components

6. Evaluation & Testing Results
6.1 Evaluation Methodology

We conducted comprehensive evaluation using multiple metrics and datasets to validate system effectiveness.

**Primary Metrics:**

1. **MRR (Mean Reciprocal Rank)**: Measures ranking quality of system answers against reference answers
   - Formula: MRR = (1/Q) Ã— Î£(1/rank_i)
   - Range: 0.0 to 1.0 (higher is better)
   - Interpretation: Average position of first correct answer

2. **MPR (Mean Percentile Rank)**: Measures percentile position of system answers  
   - Formula: MPR = (1/Q) Ã— Î£((rank_i - 1)/(R_i - 1)) Ã— 100
   - Range: 0% to 100% (higher is better)
   - Interpretation: How well system answer ranks among all candidates

3. **Latency**: End-to-end processing time per question (seconds)

4. **Success Rate**: Percentage of questions successfully processed without errors

5. **Classification Accuracy**: Correctness of question type prediction

**Dataset:**
- **Source**: Wiki-NFQA (Wikipedia Non-Factoid QA dataset)
- **Evaluation Set**: dev100 - 97 questions across 6 question types
- **Distribution**: 
  - EVIDENCE-BASED: 32 questions (33%)
  - DEBATE: 24 questions (25%)
  - REASON: 19 questions (20%)
  - INSTRUCTION: 11 questions (11%)
  - EXPERIENCE: 6 questions (6%)
  - COMPARISON: 5 questions (5%)
- **Reference Answers**: Human-annotated gold standard responses

**Evaluation Framework:**
- **Scoring Method**: LLM-based similarity evaluation using Gemini 2.5 Flash
- **Comparison**: System answers ranked against reference answers
- **Metrics Tool**: Custom LINKAGE evaluation framework (scripts/evaluate_linkage.py)
- **Reproducibility**: All evaluation commands documented in EVALUATION.md

**Systems Evaluated:**
1. **LLM-Only (Gemini 2.5 Flash)**: Baseline without retrieval
2. **LLM-Only (Llama 3.3 70B)**: Open-source baseline via Groq
3. **RAG Baseline (Gemini)**: Simple retrieval without decomposition
4. **RAG Baseline (Llama)**: Open-source RAG approach
5. **Typed-RAG (Gemini)**: Full system with type-aware decomposition
6. **Typed-RAG (Llama)**: Our approach with open-source model
6.2 Ablation Study Results
Objective: Measure the impact of each component (classification, decomposition, retrieval)
Experimental Setup:
Input: 6 questions from Wiki-NFQA dev set
Model: Meta Llama 3.2 3B Instruct
Backend: FAISS + Wikipedia passages
Variants:
Full Typed-RAG: All components enabled (baseline)
No Classification: Forced to â€œEvidence-basedâ€ type
No Decomposition: Single aspect query
No Retrieval: Pure LLM generation
6.2.1 Performance Metrics
System Variant
Success Rate
Questions
Avg Latency (s)
Speedup
Full Typed-RAG
100%
6/6
3.81
Baseline
No Classification
100%
6/6
2.81
+26% faster
No Decomposition
100%
6/6
3.27
+14% faster
No Retrieval
100%
6/6
5.62
-48% slower


Key Findings:
Classification Overhead: Adds ~1 second but enables type-aware decomposition
Retrieval Benefit: Provides 48% speedup compared to pure LLM
All Variants Stable: 100% success rate across all configurations
6.2.2 Quality Metrics (LINKAGE Evaluation)
System Variant
MRR â†‘
MPR â†‘
Quality vs Baseline
Full Typed-RAG
0.4722
70.83%
Baseline
No Classification
0.4722
70.83%
âœ“ Same
No Decomposition
0.4444
66.67%
-6% worse
No Retrieval
0.4722
70.83%
âœ“ Same

Key Findings:
Decomposition Impact: Removing decomposition reduces quality by 6% (MRR: 0.47 â†’ 0.44)
Classification Impact: No direct quality impact (enables decomposition indirectly)
Retrieval Impact: No quality impact but significantly faster
6.2.3 Quality-to-Latency Trade-off Analysis
We computed an efficiency ratio: Quality / Latency
Variant
MRR
Latency (s)
Efficiency (MRR/s)
No Classification
0.4722
2.81
0.168 â­ Best
Full Typed-RAG
0.4722
3.81
0.124
No Decomposition
0.4444
3.27
0.136
No Retrieval
0.4722
5.62
0.084

Recommendation: For production systems, â€œNo Classificationâ€ variant offers the best efficiency (26% faster with same quality)
6.2.4 Per-Question Latency Breakdown
Full Typed-RAG Latencies:
â€œWhat is quantum computing?â€ - 6.60s (Evidence)
â€œPython vs Java for web development?â€ - 3.25s (Comparison)
â€œDevelopersâ€™ experiences with Rust?â€ - 3.43s (Experience)
â€œWhy is React popular?â€ - 3.07s (Reason)
â€œHow to deploy ML model?â€ - 2.93s (Instruction)
â€œAdopt microservices?â€ - 3.60s (Debate)
Observation: Evidence-based questions are slowest (6.6s) due to LLM generation overhead, while Instruction questions are fastest (2.9s).
6.3 Classifier Performance Evaluation

Objective: Measure classification accuracy and validate hybrid approach

**Dataset**: Wiki-NFQA dev100 (97 questions with ground-truth types)

6.3.1 Overall Accuracy

**[INSERT IMAGE: Figure 6.3 - Classification Accuracy Comparison]**

| Classification Method | Accuracy | Speed | Offline Capable | Cost per Query |
|----------------------|----------|-------|-----------------|----------------|
| Pattern-Based Only | 67.01% (65/97) | <1ms | âœ“ Yes | Free |
| LLM Only (Gemini) | 100% (97/97) | 200-500ms | âœ— No | ~$0.001 |
| **Hybrid (Current)** | **91.75% (89/97)** | ~50ms avg | Hybrid | ~$0.0004 |

**Actual Results on dev100:**
- **Correct Classifications**: 89/97 questions
- **Misclassifications**: 8 questions (8.25% error rate)
- **Pattern Match Rate**: 62% of queries (60/97)
- **LLM Fallback Rate**: 38% of queries (37/97)

**Hybrid Strategy Benefits:**
- **Speed**: 62% of queries answered instantly (<1ms)
- **Cost**: 60% reduction vs pure LLM approach
- **Accuracy**: 91.75% - acceptable trade-off for speed/cost
- **Reliability**: LLM fallback ensures difficult cases are handled correctly
6.3.2 Per-Type Performance (Pattern-Based)
Question Type
Precision
Recall
F1-Score
Support
Evidence-based
0.33
1.00
0.50
1
Comparison
1.00
1.00
1.00
1
Experience
0.00
0.00
0.00
1
Reason
1.00
1.00
1.00
1
Instruction
1.00
1.00
1.00
1
Debate
0.00
0.00
0.00
1

Analysis:
Strong: Comparison, Reason, Instruction (100% F1)
Weak: Experience, Debate (0% F1 - require LLM)
Mixed: Evidence-based (low precision due to over-matching)
6.3.3 Confusion Matrix


Predicted â†’
Evidence
Comparison
Experience
Reason
Instruction
Debate
Evidence â†“
1
0
0
0
0
0


Comparison
0
1
0
0
0
0


Experience
1
0
0
0
0
0


Reason
0
0
0
1
0
0


Instruction
0
0
0
0
1
0


Debate
1
0
0
0
0
0



Error Analysis: Experience and Debate questions misclassified as Evidence-based due to lack of distinguishing keywords.
6.4 System Comparison (Typed-RAG vs Baselines)
Objective: Compare Typed-RAG against standard approaches across two models

System Configurations Evaluated:
- **LLM-Only**: Pure language model generation without retrieval
- **RAG Baseline**: Simple retrieval + generation (no decomposition)
- **Typed-RAG**: Full system with type-aware decomposition (our approach)
- **Models**: Gemini 2.5 Flash and Llama 3.3 70B (via Groq)
- **Dataset**: Wiki-NFQA dev100 (97 non-factoid questions)

6.4.1 Actual Performance Results (LINKAGE Evaluation)

**[INSERT IMAGE: Figure 6.1 - MRR and MPR Comparison Across 6 Systems]**

| System | Model | MRR â†‘ | MPR â†‘ | Questions | Description |
|--------|-------|-------|-------|-----------|-------------|
| **LLM-Only** | Llama 3.3 70B | **0.3726** | **71.90%** | 97/97 | Best overall (no retrieval) |
| LLM-Only | Gemini 2.5 Flash | 0.3332 | 61.89% | 97/97 | Commercial model baseline |
| RAG Baseline | Llama 3.3 70B | 0.2905 | 59.00% | 97/97 | Simple retrieval approach |
| **Typed-RAG** | Llama 3.3 70B | 0.2880 | 62.89% | 97/97 | Type-aware retrieval |
| Typed-RAG | Gemini 2.5 Flash | 0.2280 | 49.12% | 97/97 | Shows improvement over baseline |
| RAG Baseline | Gemini 2.5 Flash | 0.1878 | 43.95% | 97/97 | Lowest performance |

**Key Findings:**

1. **Surprising Result**: LLM-Only outperformed all RAG systems
   - Llama LLM-Only achieved **71.90% MPR** (best overall)
   - Indicates strong parametric knowledge in modern LLMs
   - Suggests retrieval quality issues may hurt performance

2. **Typed-RAG vs RAG Baseline**:
   - Gemini: **+5.17% MPR improvement** (49.12% vs 43.95%)
   - Llama: **+3.89% MPR improvement** (62.89% vs 59.00%)
   - Type-aware decomposition consistently helps

3. **Model Comparison**:
   - Llama 3.3 70B outperforms Gemini 2.5 Flash across all approaches
   - Larger model size (70B) shows benefits for complex reasoning

4. **100% Success Rate**: All systems processed all 97 questions without failures

6.4.2 Per-Question-Type Performance

**[INSERT IMAGE: Figure 6.2 - MPR Performance by Question Type]**

**Typed-RAG (Llama 3.3 70B) Performance by Type:**

| Question Type | Questions | MRR | MPR | Notes |
|--------------|-----------|-----|-----|-------|
| DEBATE | 24 | 0.4618 | **79.17%** | Strongest type |
| COMPARISON | 5 | 0.3900 | **80.00%** | Excellent multi-aspect handling |
| EVIDENCE-BASED | 32 | 0.2552 | 60.81% | Most common type |
| REASON | 19 | 0.2258 | 51.05% | Causal reasoning challenges |
| INSTRUCTION | 11 | 0.2152 | 60.10% | Step-by-step decomposition |
| EXPERIENCE | 6 | 0.1972 | 52.50% | Opinion aggregation difficult |

**Analysis:**
- **Best Performance**: DEBATE (79.17%) and COMPARISON (80.00%) questions
  - Multi-perspective analysis aligns well with decomposition strategy
  - Type-aware aspect generation produces structured outputs
  
- **Moderate Performance**: EVIDENCE-BASED (60.81%) and INSTRUCTION (60.10%)
  - Straightforward retrieval works reasonably well
  - Instruction decomposition into steps is effective
  
- **Challenging Types**: REASON (51.05%) and EXPERIENCE (52.50%)
  - Causal reasoning requires deeper inference
  - Experiential questions often lack relevant documents

6.4.3 Qualitative Comparison
Example Question: â€œPython vs Java for web development?â€
LLM-Only Output:
Python and Java are both popular for web development. Python has frameworks like Django and Flask, while Java has Spring Boot. Python is generally easier to learn, while Java offers better performance for large-scale applications.
RAG Baseline Output:
Based on the retrieved documents, Python is widely used for web development due to its simplicity and rapid development capabilities. Java is preferred in enterprise environments for its robustness and scalability. [Additional details from top-5 documents]
Typed-RAG Output:
Performance: Java typically offers better raw performance for web applications, with Spring Boot achieving higher throughput in benchmarks. Pythonâ€™s performance is adequate for most use cases but may require optimization for high-load scenarios.
Ecosystem: Python has a rich ecosystem with Django (full-featured) and Flask (micro-framework). Javaâ€™s Spring ecosystem is mature with extensive enterprise support.
Learning Curve: Pythonâ€™s syntax is more intuitive for beginners, with faster time-to-productivity. Java has steeper learning curve but enforces better practices through strong typing.
Deployment: Python apps typically use WSGI servers (Gunicorn, uWSGI). Java apps are often containerized with Spring Bootâ€™s embedded server.
Community: Both have large communities. Pythonâ€™s is broader (including data science), while Javaâ€™s is more enterprise-focused.
Analysis:
LLM-Only: Brief, lacks depth, no evidence citations
RAG Baseline: Provides evidence but lacks structure
Typed-RAG: Structured comparison across multiple axes with citations
6.5 Decomposition Strategy Validation
Objective: Verify that type-specific decomposition strategies produce appropriate sub-queries
6.5.1 Decomposition Patterns Observed
Question Type
Avg Sub-Queries
Min
Max
Strategy
Evidence-based
1.0
1
1
Passthrough
Comparison
4.2
3
5
Subject + Axes
Experience
3.5
2
4
Topical Angles
Reason
3.8
2
4
Causal Factors
Instruction
4.0
3
5
Sequential Steps
Debate
3.0
3
3
Pro + Con + Synthesis

6.5.2 Example Decompositions
Comparison Question: â€œPython vs Java for web development?â€
âœ“ Generated 5 sub-queries:
  1. Performance comparison: Python vs Java for web development
  2. Ecosystem comparison: Python vs Java web frameworks
  3. Learning curve: Python vs Java for web developers
  4. Deployment options: Python vs Java web applications
  5. Community support: Python vs Java web development
Instruction Question: â€œHow to deploy a machine learning model to production?â€
âœ“ Generated 5 sub-queries:
  1. Prerequisites for ML model deployment
  2. Model preparation and serialization steps
  3. Infrastructure setup and deployment options
  4. Monitoring and validation post-deployment
  5. Common deployment issues and troubleshooting
Validation: All decompositions follow expected patterns for their question types.
6.6 Retrieval Quality Analysis
Objective: Measure retrieval effectiveness for sub-queries
Metrics:
Documents Retrieved: Total documents across all aspects
Average Score: Similarity score (0-1)
Coverage: Do retrieved documents cover all aspects?
6.6.1 Retrieval Statistics
Question Type
Avg Docs/Question
Avg Similarity Score
Coverage
Evidence-based
5
0.78
âœ“
Comparison
25 (5 per aspect)
0.72
âœ“
Experience
18
0.70
Partial
Reason
20
0.75
âœ“
Instruction
25
0.76
âœ“
Debate
15
0.74
âœ“

Observation: All question types retrieve relevant documents with good similarity scores (0.70-0.78).
6.6.2 Reranking Impact
Experiment: Compare retrieval with and without cross-encoder reranking
Configuration
MRR
MPR
Latency (s)
No Reranking
0.45
68%
3.20
With Reranking
0.47
71%
3.81

Impact: Reranking provides +4% MRR improvement at cost of +0.6s latency
6.7 Performance Profiling

Objective: Understand latency breakdown and identify bottlenecks

**Actual System Performance (Typed-RAG with Llama 3.3 70B):**

**[INSERT IMAGE: Figure 6.4 - Component Latency Breakdown]**

6.7.1 Component-Level Latency

| Component | Avg Time (ms) | % of Total | Notes |
|-----------|--------------|------------|-------|
| Classification | 250 | 5.0% | Pattern: <1ms (62%), LLM: 200-500ms (38%) |
| Decomposition | 180 | 3.6% | Type-specific strategy selection |
| **Retrieval (total)** | **1580** | **31.4%** | 5 sub-queries Ã— ~316ms |
| - Embedding | 65 | 1.3% | BGE model (per sub-query) |
| - Vector Search | 120 | 2.4% | FAISS similarity search |
| - Reranking | 131 | 2.6% | Cross-encoder scoring |
| **Generation** | **2750** | **54.7%** | **Primary bottleneck** |
| - Aspect Generation | 2200 | 43.7% | LLM calls (5 aspects avg) |
| - Aggregation | 550 | 10.9% | Final answer synthesis |
| Cache/Overhead | 270 | 5.4% | I/O and processing |
| **Total** | **5030** | **100%** | **5.03 seconds average** |

**Key Observations:**
1. **Generation Dominates**: 54.7% of time spent in LLM API calls
   - 5 parallel aspect generations take 2.2 seconds
   - Final aggregation adds 0.55 seconds
   - Limited by API latency (not computation)

2. **Retrieval is Efficient**: 31.4% for multi-query retrieval
   - FAISS enables fast similarity search
   - Cross-encoder reranking adds minimal overhead

3. **Classification is Fast**: Only 5% overhead
   - Pattern matching handles 62% instantly
   - Hybrid approach minimizes LLM calls

**Performance Metrics:**
- **Average Latency**: 5.03 seconds/question
- **Throughput**: ~0.20 questions/second (single-threaded)
- **Peak Memory**: 480 MB (models loaded)
- **Success Rate**: 100% (97/97 questions)
6.7.2 Memory Usage
Peak Memory: 450 MB Average per Question: 75 MB
Breakdown:
BGE Model: ~180 MB (loaded once)
FAISS Index: ~150 MB (loaded once)
Runtime buffers: ~120 MB (varies per question)
Optimization: Models loaded once and reused across queries, keeping memory footprint modest.
6.7.3 Throughput
Single-threaded: 0.26 questions/second (3.81s per question) Potential with batching: ~2-3 questions/second (with batch embedding and parallel retrieval)
6.8 Correctness Verification
How we verified correctness:
Unit Tests (33+ tests):
Test each decomposition strategy with 5 questions per type
Verify sub-query count matches expected range
Check aspect names are meaningful
Validate JSON output format
Integration Tests:
End-to-end pipeline test with sample questions
Verify all stages execute without crashes
Check output artifacts (typed_plan.json, evidence_bundle.json)
Manual Inspection:
Reviewed generated answers for 6 dev questions
Checked for hallucinations (system correctly indicates â€œunable to answerâ€ when evidence is insufficient)
Verified citations match retrieved documents
Comparison to Reference Answers:
LINKAGE evaluation against human-annotated references
MRR of 0.47 indicates ~50% of our answers rank in top positions
Result: System produces structurally correct, well-formatted answers consistently.
6.9 Reproducibility
All results are fully reproducible:
# Step 1: Setup
git clone https://github.com/258-Deep-Learning/Typed-Rag.git
cd Typed-Rag
pip install -r requirements.txt
export GOOGLE_API_KEY="your-key"

# Step 2: Download data
python scripts/setup_wiki_nfqa.py --split dev6

# Step 3: Build index
python rag_cli.py build --backend faiss --source wikipedia

# Step 4: Run ablation study
python scripts/run_ablation_study.py \
  --input data/wiki_nfqa/dev6.jsonl \
  --output results/ablation/ \
  --backend faiss \
  --source wikipedia

# Step 5: Evaluate quality
python scripts/evaluate_linkage.py \
  --systems results/ablation/*.jsonl \
  --output results/linkage_evaluation.json

# Step 6: View results
cat results/ablation/summary.json | jq
Expected Runtime: ~10-15 minutes (6 questions Ã— 4 variants)
6.10 Visualizations
We generated publication-quality plots using scripts/create_evaluation_plots.py:
Ablation Latency Comparison: Bar chart showing average latency per variant
Ablation Success Rate: Success rate across variants
MRR/MPR Comparison: Side-by-side quality metrics
Classifier Performance: Per-type precision/recall
Confusion Matrix: Classification accuracy heatmap
Files: See results/plots/ directory
6.11 Key Findings Summary
Decomposition is Critical: Removing decomposition reduces quality by 6% (MRR: 0.47 â†’ 0.44)
Retrieval Provides Speed: 48% faster than pure LLM (3.81s vs 5.62s)
Classification is Optional: Can skip for 26% speedup without quality loss on simple datasets
Hybrid Classifier Works: 90-95% accuracy at fraction of LLM-only cost
Reranking Helps: +4% MRR improvement for +0.6s latency
System is Stable: 100% success rate across all variants

7. Screenshots and Diagrams
7.1 System Architecture Diagram

**[INSERT IMAGE: Figure 7.1 - High-Level System Architecture]**

Key Components:
- User Question â†’ Classifier â†’ Decomposer â†’ Retrieval Orchestrator â†’ Generator â†’ Aggregator â†’ Final Answer

7.2 UI Screenshots

**[INSERT IMAGE: Figure 7.2 - Streamlit Query Interface]**

Screenshot 1: Query Interface
Shows input box for questions
Displays classification result (e.g., â€œComparisonâ€)
Shows decomposition into sub-queries
Displays aspect-level answers
Shows final aggregated response

**[INSERT IMAGE: Figure 7.3 - Evaluation Results Dashboard]**

Screenshot 2: Evaluation Results Tab
Performance comparison table (MRR, MPR, Latency)
Ablation study results visualization
Per-type classifier metrics

**[INSERT IMAGE: Figure 7.4 - Decomposition Visualization]**

Screenshot 3: Decomposition Visualization
Shows a Comparison question split into 5 aspects
Visual representation of parallel retrieval
Evidence bundle organization by aspect
7.3 Hardware Setup
Development Workstation:
MacBook Pro / Linux Desktop
CPU: Intel/AMD (no GPU required)
RAM: 16GB
Storage: SSD for fast index loading
No Special Hardware Required: System runs entirely on CPU with API-based LLM access.
7.4 Demo Snapshots

**[INSERT IMAGE: Figure 7.5 - Complete Demo Flow Example]**

Demo Flow:
User enters: â€œPython vs Java for web development?â€
System classifies: â€œComparisonâ€
Decomposes into 5 aspects: Performance, Ecosystem, Learning Curve, Deployment, Community
Retrieves 5 documents per aspect (25 total)
Generates aspect answers
Aggregates into structured comparison
[Include screenshots from streamlit app showing each stage]

8. Commands and Scripts for Reproduction
8.1 Complete Evaluation Pipeline
#!/bin/bash
# complete_evaluation.sh - Run full evaluation pipeline

echo "ğŸ”¬ Typed-RAG Complete Evaluation Pipeline"
echo "=========================================="

# Step 1: Environment setup
echo "ğŸ“¦ Step 1/7: Setting up environment..."
pip install -r requirements.txt
export GOOGLE_API_KEY="your-gemini-api-key"

# Step 2: Download data
echo "ğŸ“‚ Step 2/7: Downloading Wiki-NFQA dataset..."
python scripts/setup_wiki_nfqa.py --split dev6

# Step 3: Build index
echo "ğŸ—ï¸  Step 3/7: Building FAISS index..."
python rag_cli.py build --backend faiss --source wikipedia

# Step 4: Run ablation study
echo "ğŸ§ª Step 4/7: Running ablation study (4 variants Ã— 6 questions)..."
python scripts/run_ablation_study.py \
  --input data/wiki_nfqa/dev6.jsonl \
  --output results/ablation/ \
  --backend faiss \
  --source wikipedia \
  --model meta-llama/Llama-3.2-3B-Instruct

# Step 5: Evaluate quality
echo "ğŸ“Š Step 5/7: Evaluating answer quality (LINKAGE metrics)..."
python scripts/evaluate_linkage.py \
  --systems results/ablation/*.jsonl \
  --output results/linkage_evaluation.json

# Step 6: Evaluate classifier
echo "ğŸ¯ Step 6/7: Evaluating classifier accuracy..."
python -m typed_rag.classifier.classifier \
  --input data/wiki_nfqa/dev6.jsonl \
  --output results/classifier_evaluation.json

# Step 7: Generate plots
echo "ğŸ“ˆ Step 7/7: Generating visualizations..."
python scripts/create_evaluation_plots.py \
  --output results/plots/

echo "âœ… Evaluation complete!"
echo "ğŸ“ Results: results/"
echo "ğŸ“Š Plots: results/plots/"
echo "ğŸ“„ Reports: markdown_results/"
Make executable and run:
chmod +x complete_evaluation.sh
./complete_evaluation.sh
Expected Runtime: ~15-20 minutes
8.2 Individual Test Commands
Test Classification:
pytest tests/test_classifier.py -v
Test Decomposition (30 tests):
pytest tests/test_decomposition.py -v
Test Generation:
pytest tests/test_generation.py -v
Run All Tests:
pytest tests/ -v --cov=typed_rag --cov-report=html
Quick Smoke Test:
python -c "
from typed_rag.classifier import classify_question
from typed_rag.decompose import decompose_question

qtype = classify_question('What is Python?', use_llm=False)
plan = decompose_question('What is Python?', qtype)
print(f'âœ“ Type: {qtype}, Sub-queries: {len(plan.sub_queries)}')
"
8.3 Interactive Demo
Start Streamlit UI:
streamlit run app.py
Access at: http://localhost:8501
Try these questions:
Evidence: â€œWhat is quantum computing?â€
Comparison: â€œPython vs Java for web development?â€
Reason: â€œWhy is React popular for frontend development?â€
Instruction: â€œHow to deploy a machine learning model?â€
Experience: â€œWhat are developersâ€™ experiences with Rust?â€
Debate: â€œShould companies adopt microservices architecture?â€
8.4 Performance Profiling
python scripts/profile_performance.py \
  --input data/wiki_nfqa/dev6.jsonl \
  --output results/performance_profile.json \
  --backend faiss \
  --source wikipedia

# View results
cat results/performance_profile.json | jq '.throughput'
cat results/performance_profile.json | jq '.memory'
cat results/performance_profile.json | jq '.component_latency'

9. References
Research Papers
Park, A., Lee, H., Nam, H., Maeng, Y., & Lee, D. (2025). Typed-RAG: Type-Aware Decomposition of Non-Factoid Questions for Retrieval-Augmented Generation. arXiv:2503.15879v3 [cs.CL].
Lewis, P., Perez, E., Piktus, A., Petroni, F., Karpukhin, V., Goyal, N., â€¦ & Kiela, D. (2020). Retrieval-augmented generation for knowledge-intensive nlp tasks. Advances in Neural Information Processing Systems, 33, 9459-9474.
Izacard, G., & Grave, E. (2021). Leveraging passage retrieval with generative models for open domain question answering. In Proceedings of EACL 2021.
Bolotova, V., Blinov, V., Scholer, F., Croft, W. B., & Sanderson, M. (2022). A non-factoid question answering taxonomy. In Proceedings of SIGIR 2022.
Yang, P., & Alonso, O. (2024). Understanding user information needs in conversational search. Information Processing & Management.
Models and Embeddings
BAAI. (2023). BGE (BAAI General Embedding). https://github.com/FlagOpen/FlagEmbedding
Model: BAAI/bge-small-en-v1.5
Paper: â€œC-Pack: Packaged Resources To Advance General Chinese Embeddingâ€
Google. (2024). Gemini API. https://ai.google.dev/
Model: Gemini 2.0 Flash Lite
Documentation: https://ai.google.dev/gemini-api/docs
Meta. (2024). Llama 3.2 Models. https://ai.meta.com/llama/
Model: Llama-3.2-3B-Instruct
Used for ablation studies
Libraries and Frameworks
Johnson, J., Douze, M., & JÃ©gou, H. (2019). Billion-scale similarity search with GPUs. IEEE Transactions on Big Data, 7(3), 535-547.
FAISS: https://github.com/facebookresearch/faiss
Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., â€¦ & Rush, A. M. (2020). Transformers: State-of-the-art natural language processing. In Proceedings of EMNLP 2020.
HuggingFace Transformers: https://github.com/huggingface/transformers
Reimers, N., & Gurevych, I. (2019). Sentence-BERT: Sentence embeddings using Siamese BERT-networks. In Proceedings of EMNLP-IJCNLP 2019.
Sentence-Transformers: https://www.sbert.net/
LangChain. (2024). LangChain Documentation. https://python.langchain.com/
Used for document loading and text splitting
Pinecone Systems. (2024). Pinecone Vector Database. https://www.pinecone.io/
Cloud vector store alternative to FAISS
Datasets
Wikipedia. Wikipedia Dump 2024. https://dumps.wikimedia.org/
Source for passage collection
Wiki-NFQA Dataset. (2025). Wikipedia Non-Factoid Question Answering Dataset.
Derived from Wikipedia articles
6 question types with human annotations
Tutorials and Documentation
Streamlit. (2024). Streamlit Documentation. https://docs.streamlit.io/
Web UI framework
Pytest. (2024). Pytest Documentation. https://docs.pytest.org/
Testing framework
Matplotlib & Seaborn. (2024). Scientific Visualization in Python.
https://matplotlib.org/
https://seaborn.pydata.org/
Related Work on Query Decomposition
Zhou, D., SchÃ¤rli, N., Hou, L., Wei, J., Scales, N., Wang, X., â€¦ & Chi, E. (2023). Least-to-most prompting enables complex reasoning in large language models. In Proceedings of ICLR 2023.
Press, O., Zhang, M., Min, S., Schmidt, L., Goodman, N., & Zettlemoyer, L. (2022). Measuring and narrowing the compositionality gap in language models. arXiv preprint arXiv:2210.03350.
An, S., Ma, Z., Feng, S., Sitaram, S., & Cardie, C. (2024). Type-specific approaches to non-factoid question answering. In Proceedings of NAACL 2024.
Evaluation Frameworks
Thorne, J., Vlachos, A., Christodoulopoulos, C., & Mittal, A. (2018). FEVER: a large-scale dataset for fact extraction and VERification. In Proceedings of NAACL 2018.
Inspiration for LINKAGE evaluation framework
Rajpurkar, P., Zhang, J., Lopyrev, K., & Liang, P. (2016). SQuAD: 100,000+ questions for machine comprehension of text. In Proceedings of EMNLP 2016.
Standard QA benchmark for comparison

Appendix A: Additional Technical Details
A.1 Caching Implementation
Cache Key Generation:
import hashlib
import json

def generate_cache_key(question: str, config: dict) -> str:
    cache_input = {
        "question": question,
        "config": config
    }
    cache_str = json.dumps(cache_input, sort_keys=True)
    return hashlib.md5(cache_str.encode()).hexdigest()
Cache Storage:
Format: JSON files
Location: cache/{component}/{hash}.json
TTL: No expiration (manual cleanup)
A.2 Error Handling Strategy
Multi-level Fallbacks:
LLM API Failure: Fall back to template-based generation
Vector Store Unavailable: Return error with helpful message
Empty Retrieval Results: Generate answer indicating insufficient information
Invalid Question Type: Default to â€œEvidence-basedâ€
A.3 Prompt Engineering
Classification Prompt:
You are a question type classifier. Classify the following question into one of these types:
- Evidence-based: Seeks factual information
- Comparison: Compares two or more entities
- Experience: Asks about user experiences or opinions
- Reason: Asks why something happens
- Instruction: Asks how to do something
- Debate: Asks for pros and cons

Question: {question}

Type:
Generation Prompt (Aspect-Level):
Answer the following question based on the provided evidence snippets.
Focus on the aspect: {aspect}

Question: {question}
Sub-query: {sub_query}

Evidence:
{evidence_snippets}

Provide a clear, concise answer with citations [1], [2], etc.
If information is insufficient, state this clearly.

Answer:

Appendix B: Team Member Details

**Indraneel Sarode**
- SJSU ID: 018305092
- Email: indraneel.sarode@sjsu.edu
- Primary Role: Classification & Decomposition Logic
- Key Contributions: 
  - Hybrid classifier with 91.75% accuracy
  - Type-aware decomposition for 6 question types
  - Pattern matching system with 60+ regex patterns
  - LLM prompt engineering for classification and decomposition
  - Aspect extraction algorithms
- Lines of Code: ~930 lines
- Modules: classifier.py, query_decompose.py, prompts.py

**Nishan Paudel**
- SJSU ID: 018280561
- Email: nishan.paudel@sjsu.edu
- Primary Role: Retrieval Engine & Vector Search
- Key Contributions:
  - BGE embedding model integration
  - FAISS and Pinecone vector store implementations
  - Multi-query retrieval orchestration
  - Cross-encoder reranking system
  - Retrieval optimization and caching
- Lines of Code: ~1,080 lines
- Modules: pipeline.py, orchestrator.py, build_faiss.py, build_pinecone.py

**Vatsal Gandhi**
- SJSU ID: 018274841
- Email: vatsal.gandhi@sjsu.edu
- Primary Role: System Integration, Evaluation & UI
- Key Contributions:
  - End-to-end pipeline architecture and integration
  - LINKAGE evaluation framework (MRR, MPR metrics)
  - Ablation study with 4 variants
  - Streamlit web interface and CLI tools
  - Complete evaluation (582 answers across 6 systems)
  - Performance profiling and visualization
- Lines of Code: ~4,280 lines
- Modules: rag_system.py, eval/, app.py, rag_cli.py, scripts/

Appendix C: Code Repository Details

**GitHub Repository**: https://github.com/258-Deep-Learning/Typed-Rag

**Repository Structure:**
Main Branch: Stable release
Dev Branch: Development work
Tag: v1.0-submission for final submission
Important Files:
README_TYPED_RAG.md - Main documentation
EVALUATION.md - Evaluation guide
TESTING.md - Testing guide
requirements.txt - Dependencies
PROJECT_REPORT.md - This report
Credentials Note:
âš ï¸ NO API keys or credentials are committed to the repository
All sensitive information should be set via environment variables
Example: export GOOGLE_API_KEY="your-key"

End of Project Report
